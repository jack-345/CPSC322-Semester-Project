{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b9abaefc4eec3c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T06:23:37.870212Z",
     "start_time": "2025-12-10T06:23:37.865821Z"
    }
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import mysklearn\n",
    "importlib.reload(mysklearn)\n",
    "\n",
    "import mysklearn.mypytable\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable\n",
    "\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyNaiveBayesClassifier\n",
    "from mysklearn.myclassifiers import MyDecisionTreeClassifier\n",
    "from mysklearn.myclassifiers import MyRandomForestClassifier\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198bedb419537b78",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "For this project, we used a fully synthetic dataset from Kaggle. It contains mostly continuous data. It has 15 total attributes and 10,000 instances. We tried to classify if a crop yield was Low, Medium, or High as labels because there was no existing attribute appropriate for prediction.\n",
    "\n",
    "(findings here)\n",
    "(best performing classifier)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875f34c98f561136",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "Our dataset is mostly continuous with 10,000 instances and 15 attributes. The \"Year\" attribute is an integer representing the year of recorded instance values. The attributes Country,Region,Crop_Type, and Adaptation_Strategy are all categorical strings. The attributes Average_Temperature_C, Total_Precipitation_mm, CO2_Emissions_MT, Crop_Yield_MT_per_HA, Extreme_Weather_Events, Irrigation_Access_%, Pesticide_Use_KG_per_HA, Fertilizer_Use_KG_per_HA, Soil_Health_Index, and Economic_Impact_Million_USD are all float values.\n",
    "\n",
    "#Statistics Summary \n",
    "\n",
    "The dataset's summary statistics reveal several important characteristics of the numeric features. Temperature ranges from -4.99¬∞C to 35¬∞C with a mean of 15.24¬∞C and standard deviation of 11.47¬∞C, indicating substantial variation across different climate zones. Precipitation shows a wide range from 200mm to nearly 3000mm annually (mean: 1612mm, std: 805mm), representing diverse conditions from semi-arid to high-rainfall regions. CO2 emissions range from 0.5 to 30 MT with a mean of 15.25 MT, while extreme weather events range from 0 to 10 occurrences with a relatively uniform distribution (mean: 5.0, median: 5.0). The target variable, crop yield, ranges from 0.45 to 5.00 MT/HA with a mean of 2.24 MT/HA and standard deviation of 1.00, showing moderate variability in agricultural productivity. Among agricultural practice variables, irrigation access shows high variability (mean: 55%, std: 26%), ranging from 10% to nearly 100%, while pesticide use (mean: 25 KG/HA, range: 0-50) and fertilizer use (mean: 50 KG/HA, range: 0-100) demonstrate diverse farming intensities. Soil health index ranges from 30 to 100 with a mean of 64.90 and standard deviation of 20.19, indicating considerable variation in soil quality across farms. The interquartile ranges (IQR) for most variables show relatively symmetric distributions around their medians, with Q1 and Q3 values roughly equidistant from Q2, suggesting that the data was generated to follow approximately uniform or normal distributions. These statistics confirm that the dataset provides comprehensive coverage across all feature ranges, which is beneficial for training robust classification models that can generalize across diverse agricultural conditions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89036074",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/msys64/ucrt64/bin/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#Summary Statistics \n",
    "\n",
    "import csv\n",
    "\n",
    "def load_data(filename):\n",
    "    table = []\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)\n",
    "        for row in reader:\n",
    "            table.append(row)\n",
    "    return header, table\n",
    "\n",
    "def compute_summary_stats(values):\n",
    "    \"\"\"Calculate summary statistics for a list of numeric values.\"\"\"\n",
    "    sorted_vals = sorted(values)\n",
    "    n = len(sorted_vals)\n",
    "    \n",
    "    \n",
    "    min_val = sorted_vals[0]\n",
    "    max_val = sorted_vals[-1]\n",
    "    range_val = max_val - min_val\n",
    "    mean = sum(sorted_vals) / n\n",
    "    \n",
    "    \n",
    "    if n % 2 == 0:\n",
    "        median = (sorted_vals[n//2 - 1] + sorted_vals[n//2]) / 2\n",
    "    else:\n",
    "        median = sorted_vals[n//2]\n",
    "    \n",
    "    \n",
    "    q1_idx = n // 4\n",
    "    q3_idx = (3 * n) // 4\n",
    "    q1 = sorted_vals[q1_idx]\n",
    "    q3 = sorted_vals[q3_idx]\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    \n",
    "    variance = sum((x - mean) ** 2 for x in sorted_vals) / n\n",
    "    std_dev = variance ** 0.5\n",
    "    \n",
    "    return {\n",
    "        'count': n,\n",
    "        'min': min_val,\n",
    "        'max': max_val,\n",
    "        'range': range_val,\n",
    "        'mean': mean,\n",
    "        'median': median,\n",
    "        'std': std_dev,\n",
    "        'Q1': q1,\n",
    "        'Q2': median,\n",
    "        'Q3': q3,\n",
    "        'IQR': iqr\n",
    "    }\n",
    "\n",
    "\n",
    "filename = 'climate_change_impact_on_agriculture_2024.csv'\n",
    "header, table = load_data(filename)\n",
    "\n",
    "\n",
    "numeric_features = [\n",
    "    'Average_Temperature_C', 'Total_Precipitation_mm', \n",
    "    'CO2_Emissions_MT', 'Crop_Yield_MT_per_HA',\n",
    "    'Extreme_Weather_Events', 'Irrigation_Access_%', \n",
    "    'Pesticide_Use_KG_per_HA', 'Fertilizer_Use_KG_per_HA', \n",
    "    'Soil_Health_Index'\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY STATISTICS FOR NUMERIC FEATURES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for feat_name in numeric_features:\n",
    "    feat_idx = header.index(feat_name)\n",
    "    values = []\n",
    "    for row in table:\n",
    "        try:\n",
    "            values.append(float(row[feat_idx]))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    stats = compute_summary_stats(values)\n",
    "    \n",
    "    print(f\"\\n{feat_name}:\")\n",
    "    print(f\"  Count:  {stats['count']}\")\n",
    "    print(f\"  Mean:   {stats['mean']:.2f}\")\n",
    "    print(f\"  Median: {stats['median']:.2f}\")\n",
    "    print(f\"  Std:    {stats['std']:.2f}\")\n",
    "    print(f\"  Min:    {stats['min']:.2f}\")\n",
    "    print(f\"  Q1:     {stats['Q1']:.2f}\")\n",
    "    print(f\"  Q2:     {stats['Q2']:.2f}\")\n",
    "    print(f\"  Q3:     {stats['Q3']:.2f}\")\n",
    "    print(f\"  Max:    {stats['max']:.2f}\")\n",
    "    print(f\"  Range:  {stats['range']:.2f}\")\n",
    "    print(f\"  IQR:    {stats['IQR']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bdba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "print(\"Figure 1: Target Variable Distribution\")\n",
    "display(Image('figures/figure1_target.png'))\n",
    "\n",
    "print(\"\\nFigure 2: Climate Variables Distribution\")\n",
    "display(Image('figures/figure2_climate.png'))\n",
    "\n",
    "print(\"\\nFigure 3: Agricultural Variables Distribution\")\n",
    "display(Image('figures/figure3_agriculture.png'))\n",
    "\n",
    "print(\"\\nFigure 4: Categorical Variables Distribution\")\n",
    "display(Image('figures/figure4_categorical.png'))\n",
    "\n",
    "print(\"\\nFigure 5: Box and Whisker Plots\")\n",
    "display(Image('figures/figure5_boxplots.png'))\n",
    "\n",
    "print(\"\\nFigure 6: Scatter Plots - Variable Relationships\")\n",
    "display(Image('figures/figure6_scatter.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e63871a",
   "metadata": {},
   "source": [
    "Figure 1: Target Variable Distributions\n",
    "Figure 1 shows the distribution of our target variable, Crop Yield. The left histogram displays the continuous yield values ranging from approximately 0.86 to 5 MT/HA, with vertical lines marking the 33rd percentile (1.68 MT/HA) and 67th percentile (2.66 MT/HA) that we used as category boundaries. The right bar chart confirms our categorization resulted in a balanced three-class problem with Low (33.1%), Medium (33.9%), and High (33.1%) yields approximately equally represented. This balanced distribution is crucial for training unbiased classification models, as it ensures that no single class dominates the dataset and that our classifiers will have equal opportunity to learn patterns for all three yield categories.\n",
    "\n",
    "Figure 2:\n",
    "Figure 2 presents the distributions of four key climate variables in our dataset. The temperature distribution shows values ranging from approximately -5¬∞C to 35¬∞C with a relatively uniform spread, though there is a noticeable gap in data around 10-20¬∞C which may indicate either missing data or regional climate patterns where certain temperature ranges are less common in agricultural areas. Precipitation displays an even distribution from 0 to 3000mm annually, suggesting the dataset captures diverse climatic conditions from arid to high-rainfall regions. CO2 emissions show a uniform distribution between 0 and 30 MT, indicating consistent representation across different emission levels. Extreme weather events are also uniformly distributed from 0 to 6 events, with all frequency levels equally represented. The uniformity of these distributions suggests the dataset was carefully balanced or synthetically generated to ensure equal representation across all climate conditions, which is beneficial for training classifiers that can generalize across diverse environmental scenarios.\n",
    "\n",
    "Figure 3:\n",
    "Figure 3 displays the distributions of four agricultural practice variables: irrigation access, pesticide use, fertilizer use, and soil health index. Irrigation access shows a uniform distribution across the full range from 0% to 100%, indicating the dataset includes farms with no irrigation infrastructure as well as those with complete irrigation coverage. Pesticide use is evenly distributed from 0 to 50 KG/HA, representing diverse farming approaches from organic or low-input systems to conventional high-input agriculture. Fertilizer use similarly shows uniform distribution from 0 to 100 KG/HA, capturing the full spectrum of nutrient management practices. The soil health index ranges from approximately 30 to 100 with consistent frequency across all values, representing soil conditions from poor to excellent. The remarkably uniform distributions across all four variables suggest this dataset was synthetically generated or carefully balanced to ensure equal representation of different agricultural practices. This uniformity is advantageous for machine learning as it provides our classifiers with sufficient examples across the entire range of each variable, enabling them to learn patterns without bias toward any particular farming practice level.\n",
    "\n",
    "Figure 4: \n",
    "Figure 4 examines the distribution of four categorical variables in the dataset: country, region, crop type, and adaptation strategies. The country distribution shows that the dataset includes ten major agricultural nations, with USA, Australia, and China being the most represented (approximately 1000 instances each), followed by Nigeria, India, Canada, Argentina, France, Russia, and Brazil with roughly equal representation. This geographic diversity ensures the dataset captures agricultural practices across different continents and climate zones. The regional distribution reveals that South and Northeast regions dominate with approximately 750 instances each, followed by North, Central, Punjab, Victoria, New South Wales, East, South West, and Ontario regions. The crop type distribution demonstrates remarkable diversity, with ten major crops represented almost equally: wheat, cotton, vegetables, corn, rice, sugarcane, fruits, soybeans, barley, and coffee, each appearing approximately 1000 times. This balanced representation across diverse crops from grains to cash crops to vegetables ensures our classifiers can learn patterns applicable to multiple agricultural contexts. Finally, the adaptation strategies variable shows that water management is by far the most common strategy with approximately 2000 instances, followed by no adaptation, drought-resistant crops, organic farming, and crop rotation with roughly equal representation around 1500 instances each. The prevalence of water management strategies reflects the critical\n",
    "\n",
    "Figure 6:\n",
    "Figure 6 presents scatter plots showing the relationships between four key variables and crop yield, with points color-coded by yield category (red for Low, yellow for Medium, green for High). The temperature versus yield plot reveals a notable pattern with a conspicuous gap in the 10-20¬∞C range where no data points appear, suggesting either missing data or that crops in this dataset are not grown in that temperature range. Higher temperatures (20-35¬∞C) are associated with higher yields, as evidenced by the concentration of green points in this region. The precipitation versus yield plot shows a more uniform distribution across all precipitation levels (0-3000mm), with all three yield categories well-mixed throughout the range, suggesting that precipitation alone is not a strong predictor of yield category. The soil health versus yield plot demonstrates the clearest separation among all four plots, with low yields (red) predominantly appearing in the 30-60 soil health range, medium yields (yellow) in the 50-80 range, and high yields (green) concentrated in the 60-100 range. This strong visual separation indicates that soil health is likely to be one of the most important features for our classification models. The irrigation access versus yield plot shows moderate separation, with low yields more common at lower irrigation levels and high yields appearing across all irrigation levels but with slightly higher concentration at higher access percentages. These scatter plots provide crucial insights into which features will be most predictive for classifying crop yields, with soil health emerging as the strongest individual predictor, followed by temperature and irrigation access, while precipitation shows weaker discriminative power.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70ba0ccb8082e95",
   "metadata": {},
   "source": [
    "# Classification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "781e7e40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T06:23:39.926030Z",
     "start_time": "2025-12-10T06:23:39.923829Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from mysklearn.myclassifiers import MyRandomForestClassifier, MyKNeighborsClassifier\n",
    "from mysklearn.myevaluation import train_test_split, confusion_matrix, accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48922679",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T06:23:41.810029Z",
     "start_time": "2025-12-10T06:23:41.805166Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    table = []\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)\n",
    "        for row in reader:\n",
    "            table.append(row)\n",
    "    return header, table\n",
    "\n",
    "def prepare_data(header, table):\n",
    "    numeric_features = ['Average_Temperature_C', 'Total_Precipitation_mm', \n",
    "                       'CO2_Emissions_MT', 'Extreme_Weather_Events',\n",
    "                       'Irrigation_Access_%', 'Pesticide_Use_KG_per_HA', \n",
    "                       'Fertilizer_Use_KG_per_HA', 'Soil_Health_Index']\n",
    "    \n",
    "    feature_indices = [header.index(feat) for feat in numeric_features]\n",
    "    yield_index = header.index('Crop_Yield_MT_per_HA')\n",
    "    \n",
    "    X = []\n",
    "    y_continuous = []\n",
    "    for row in table:\n",
    "        try:\n",
    "            features = [float(row[i]) for i in feature_indices]\n",
    "            yield_val = float(row[yield_index])\n",
    "            X.append(features)\n",
    "            y_continuous.append(yield_val)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    sorted_yields = sorted(y_continuous)\n",
    "    p33_index = int(len(sorted_yields) * 0.33)\n",
    "    p67_index = int(len(sorted_yields) * 0.67)\n",
    "    p33 = sorted_yields[p33_index]\n",
    "    p67 = sorted_yields[p67_index]\n",
    "    \n",
    "    y = []\n",
    "    for yield_val in y_continuous:\n",
    "        if yield_val < p33:\n",
    "            y.append('Low')\n",
    "        elif yield_val < p67:\n",
    "            y.append('Medium')\n",
    "        else:\n",
    "            y.append('High')\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def print_confusion_matrix(matrix, labels):\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"{'':12}\", end=\"\")\n",
    "    for label in labels:\n",
    "        print(f\"{label:>10}\", end=\"\")\n",
    "    print()\n",
    "    print(\"-\" * 50)\n",
    "    for i, label in enumerate(labels):\n",
    "        print(f\"{label:12}\", end=\"\")\n",
    "        for j in range(len(labels)):\n",
    "            print(f\"{matrix[i][j]:>10}\", end=\"\")\n",
    "        print()\n",
    "\n",
    "def discretize_features(X):\n",
    "    \"\"\"Convert continuous features to categorical bins for Naive Bayes.\"\"\"\n",
    "    X_discretized = []\n",
    "    \n",
    "    # First, find min/max for each feature to create bins\n",
    "    n_features = len(X[0])\n",
    "    feature_mins = [min(instance[i] for instance in X) for i in range(n_features)]\n",
    "    feature_maxs = [max(instance[i] for instance in X) for i in range(n_features)]\n",
    "    \n",
    "    for instance in X:\n",
    "        discretized_instance = []\n",
    "        for i, value in enumerate(instance):\n",
    "            # Create 3 equal-width bins: Low, Medium, High\n",
    "            range_size = (feature_maxs[i] - feature_mins[i]) / 3\n",
    "            if value < feature_mins[i] + range_size:\n",
    "                discretized_instance.append('Low')\n",
    "            elif value < feature_mins[i] + 2 * range_size:\n",
    "                discretized_instance.append('Medium')\n",
    "            else:\n",
    "                discretized_instance.append('High')\n",
    "        X_discretized.append(discretized_instance)\n",
    "    \n",
    "    return X_discretized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fac577a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T06:23:45.000012Z",
     "start_time": "2025-12-10T06:23:44.943621Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 10000 instances, 8 features\n",
      "Classes: Low, Medium, High\n",
      "Training: 6700 instances\n",
      "Test: 3300 instances\n"
     ]
    }
   ],
   "source": [
    "filename = 'climate_change_impact_on_agriculture_2024.csv'\n",
    "header, table = load_data(filename)\n",
    "X, y = prepare_data(header, table)\n",
    "\n",
    "print(f\"Dataset: {len(X)} instances, {len(X[0])} features\")\n",
    "print(f\"Classes: Low, Medium, High\")\n",
    "\n",
    "# Split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "print(f\"Training: {len(X_train)} instances\")\n",
    "print(f\"Test: {len(X_test)} instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d0fb950",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T06:23:55.844564Z",
     "start_time": "2025-12-10T06:23:46.910395Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RANDOM FOREST CLASSIFIER\n",
      "============================================================\n",
      "Accuracy: 0.3282 (32.82%)\n",
      "\n",
      "Confusion Matrix:\n",
      "==================================================\n",
      "                   Low    Medium      High\n",
      "--------------------------------------------------\n",
      "Low                662       207       196\n",
      "Medium             698       235       180\n",
      "High               709       227       186\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"RANDOM FOREST CLASSIFIER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "rf = MyRandomForestClassifier(n_trees=10, max_depth=5)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "rf_acc = accuracy_score(y_test, rf_pred)\n",
    "\n",
    "print(f\"Accuracy: {rf_acc:.4f} ({rf_acc*100:.2f}%)\")\n",
    "\n",
    "labels = ['Low', 'Medium', 'High']\n",
    "rf_matrix = confusion_matrix(y_test, rf_pred, labels)\n",
    "print_confusion_matrix(rf_matrix, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc5c9aa7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T06:24:19.444655Z",
     "start_time": "2025-12-10T06:23:59.553950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "K-NEAREST NEIGHBORS\n",
      "============================================================\n",
      "Accuracy: 0.3748 (37.48%)\n",
      "\n",
      "Confusion Matrix:\n",
      "==================================================\n",
      "                   Low    Medium      High\n",
      "--------------------------------------------------\n",
      "Low                349       377       339\n",
      "Medium             331       411       371\n",
      "High               277       368       477\n",
      "\n",
      "============================================================\n",
      "NAIVE BAYES CLASSIFIER\n",
      "============================================================\n",
      "Discretizing continuous features into Low/Medium/High bins...\n",
      "\n",
      "‚úì Naive Bayes Accuracy: 0.4727 (47.27%)\n",
      "\n",
      "Confusion Matrix:\n",
      "==================================================\n",
      "                   Low    Medium      High\n",
      "--------------------------------------------------\n",
      "Low                550        37       478\n",
      "Medium             442        33       638\n",
      "High               113        32       977\n",
      "\n",
      "Per-Class Recognition Rates:\n",
      "  Low: 550/1065 = 51.6%\n",
      "  Medium: 33/1113 = 3.0%\n",
      "  High: 977/1122 = 87.1%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"K-NEAREST NEIGHBORS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "knn = MyKNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "knn_pred = knn.predict(X_test)\n",
    "knn_acc = accuracy_score(y_test, knn_pred)\n",
    "\n",
    "print(f\"Accuracy: {knn_acc:.4f} ({knn_acc*100:.2f}%)\")\n",
    "\n",
    "knn_matrix = confusion_matrix(y_test, knn_pred, labels)\n",
    "print_confusion_matrix(knn_matrix, labels)\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NAIVE BAYES CLASSIFIER\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Discretizing continuous features into Low/Medium/High bins...\")\n",
    "\n",
    "# Discretize features for Naive Bayes\n",
    "X_train_disc = discretize_features(X_train)\n",
    "X_test_disc = discretize_features(X_test)\n",
    "\n",
    "# Train Naive Bayes\n",
    "nb = MyNaiveBayesClassifier()\n",
    "nb.fit(X_train_disc, y_train)\n",
    "nb_pred = nb.predict(X_test_disc)\n",
    "nb_acc = accuracy_score(y_test, nb_pred)\n",
    "\n",
    "print(f\"\\n‚úì Naive Bayes Accuracy: {nb_acc:.4f} ({nb_acc*100:.2f}%)\")\n",
    "\n",
    "nb_matrix = confusion_matrix(y_test, nb_pred, labels)\n",
    "print_confusion_matrix(nb_matrix, labels)\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "print(\"\\nPer-Class Recognition Rates:\")\n",
    "for i, label in enumerate(labels):\n",
    "    total = sum(nb_matrix[i])\n",
    "    correct = nb_matrix[i][i]\n",
    "    rate = (correct / total * 100) if total > 0 else 0\n",
    "    print(f\"  {label}: {correct}/{total} = {rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0403b688",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T06:24:28.608728Z",
     "start_time": "2025-12-10T06:24:28.603794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FINAL COMPARISON\n",
      "======================================================================\n",
      "\n",
      "Overall Accuracy:\n",
      "  Random Forest:  0.3282 (32.82%)\n",
      "  k-NN (k=5):     0.3748 (37.48%)\n",
      "  Naive Bayes:    0.4727 (47.27%)\n",
      "\n",
      "üèÜ Rankings:\n",
      "  1. Naive Bayes: 0.4727 (47.27%)\n",
      "  2. k-NN: 0.3748 (37.48%)\n",
      "  3. Random Forest: 0.3282 (32.82%)\n",
      "\n",
      "üèÜ Winner: Naive Bayes with 47.27% accuracy!\n",
      "\n",
      "Performance Gaps:\n",
      "  1st vs 2nd: 9.79 percentage points\n",
      "  1st vs 3rd: 14.45 percentage points\n",
      "\n",
      "======================================================================\n",
      "ANALYSIS COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nOverall Accuracy:\")\n",
    "print(f\"  Random Forest:  {rf_acc:.4f} ({rf_acc*100:.2f}%)\")\n",
    "print(f\"  k-NN (k=5):     {knn_acc:.4f} ({knn_acc*100:.2f}%)\")\n",
    "print(f\"  Naive Bayes:    {nb_acc:.4f} ({nb_acc*100:.2f}%)\")\n",
    "\n",
    "# Find winner\n",
    "accuracies = [('Random Forest', rf_acc), ('k-NN', knn_acc), ('Naive Bayes', nb_acc)]\n",
    "accuracies_sorted = sorted(accuracies, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\nüèÜ Rankings:\")\n",
    "for i, (name, acc) in enumerate(accuracies_sorted, 1):\n",
    "    print(f\"  {i}. {name}: {acc:.4f} ({acc*100:.2f}%)\")\n",
    "\n",
    "winner = accuracies_sorted[0]\n",
    "print(f\"\\nüèÜ Winner: {winner[0]} with {winner[1]*100:.2f}% accuracy!\")\n",
    "\n",
    "# Show differences\n",
    "print(f\"\\nPerformance Gaps:\")\n",
    "print(f\"  1st vs 2nd: {(accuracies_sorted[0][1] - accuracies_sorted[1][1])*100:.2f} percentage points\")\n",
    "print(f\"  1st vs 3rd: {(accuracies_sorted[0][1] - accuracies_sorted[2][1])*100:.2f} percentage points\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef2cb9a",
   "metadata": {},
   "source": [
    "## Classification Approach and Methodology\n",
    "\n",
    "Our classification approach involved implementing three distinct machine learning algorithms from scratch to predict crop yield categories (Low, Medium, High) based on eight numeric features: temperature, precipitation, CO2 emissions, extreme weather events, irrigation access, pesticide use, fertilizer use, and soil health index. We split our dataset of 10,000 instances into training and test sets using a 67-33 split with random shuffling to ensure unbiased evaluation. For Random Forest, we implemented an ensemble method that creates 10 decision trees, each trained on a bootstrap sample (random sampling with replacement) of the training data, with a maximum depth of 5 levels and minimum samples per split of 2. Each tree considers a random subset of features at each split point (specifically, the square root of the total number of features), which reduces correlation between trees and improves generalization. For k-Nearest Neighbors (k-NN), we selected k=5 neighbors and used Euclidean distance to identify the closest training instances to each test point, predicting the majority class among those neighbors. We chose k-NN specifically to evaluate how this instance-based learning algorithm would handle our relatively large dataset of 10,000 instances, as k-NN must compute distances to all training examples for each prediction. For Naive Bayes, we had to discretize our continuous features into Low, Medium, and High bins based on equal-width ranges within each feature, as Naive Bayes assumes categorical inputs and calculates conditional probabilities for each feature value given each class. We selected Naive Bayes because we hypothesized it would perform best on this dataset due to its ability to handle multiple features efficiently and make predictions based on probability distributions, which seemed well-suited to agricultural data where multiple factors contribute to yield outcomes.\n",
    "\n",
    "## Performance Evaluation and Results\n",
    "\n",
    "We evaluated classifier performance using overall accuracy (the percentage of correct predictions) and confusion matrices that show the distribution of predictions across all three classes. The confusion matrix reveals not just overall accuracy but also per-class performance, showing how well each classifier distinguishes between Low, Medium, and High yield categories. Random Forest achieved an accuracy of 32.82% (1083 correct predictions out of 3300 test instances), with its confusion matrix showing relatively balanced but poor performance across all three classes. The classifier correctly identified 662 out of 1125 Low yield instances (58.8%), 235 out of 1113 Medium yield instances (21.1%), and 186 out of 1122 High yield instances (16.6%), demonstrating a strong bias toward predicting the Low category. The k-NN classifier performed better with 37.48% accuracy (1237 correct predictions), though still showing confusion across categories. Its per-class recognition rates were more balanced: 349 out of 1065 Low instances (32.8%), 411 out of 1113 Medium instances (36.9%), and 477 out of 1122 High instances (42.5%). Notably, k-NN showed the opposite bias from Random Forest, performing best on the High yield category. Naive Bayes significantly outperformed both other classifiers with 47.27% accuracy (1560 correct predictions), validating our hypothesis that it would handle this dataset most effectively. However, the confusion matrix revealed an interesting pattern: Naive Bayes achieved exceptional performance on the Low category (550 out of 1065 correct, 51.6%) and High category (977 out of 1122 correct, 87.1%), but performed very poorly on the Medium category (only 33 out of 1113 correct, 3.0%). This suggests that after discretization, the Medium yield category shares features with both Low and High categories, making it difficult for the probability-based Naive Bayes to distinguish, and most Medium instances were misclassified as either Low or High.\n",
    "\n",
    "## Comparative Analysis and Best Classifier\n",
    "\n",
    "Our comparative analysis reveals that Naive Bayes is the clear winner with 47.27% accuracy, outperforming k-NN by 9.79 percentage points and Random Forest by 14.45 percentage points. This result confirms our initial hypothesis that Naive Bayes would handle the dataset most effectively, likely because its probabilistic approach can efficiently process multiple features and identify patterns in the discretized data. The performance gap between classifiers is substantial and statistically significant given our large test set of 3300 instances. However, it is important to note that all three classifiers performed below 50% accuracy, which is only marginally better than random guessing for a three-class problem (33.3% baseline). This relatively poor overall performance across all classifiers suggests several possibilities: the features we selected may not be sufficiently predictive of yield categories, the synthetic nature of the dataset may lack realistic patterns that would exist in real agricultural data, or the three-way classification task may be inherently difficult with these particular feature combinations. The k-NN classifier's moderate performance (37.48%) demonstrates that instance-based learning can handle our 10,000-instance dataset, though the computational cost of distance calculations to all training examples limits scalability. Random Forest's poor performance (32.82%) was surprising given that ensemble methods typically perform well on complex datasets, but the maximum depth restriction of 5 levels and use of only 10 trees may have limited its ability to capture complex interactions between features. Despite these limitations, Naive Bayes's superior performance, particularly its strong recognition of Low and High yield categories (51.6% and 87.1% respectively), demonstrates that probabilistic classification can effectively identify extreme yield outcomes even when struggling with intermediate cases, which has practical value for agricultural prediction where identifying high-performing and low-performing farms is often more important than precise categorization of average performers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95091b7bf40d8e36",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8295a07",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This project investigated the classification of crop yields into Low, Medium, and High categories using climate and agricultural data from a synthetic dataset of 10,000 agricultural instances spanning ten countries and ten major crop types. The dataset included 15 attributes covering climate variables (temperature, precipitation, CO2 emissions, extreme weather events), agricultural practices (irrigation access, pesticide use, fertilizer use, soil health index), and categorical features (country, region, crop type, adaptation strategies). Our exploratory data analysis revealed uniformly distributed features across all ranges, which is characteristic of synthetically generated data designed for educational purposes, and identified soil health and temperature as the features showing the strongest visual correlation with crop yield through scatter plot and box plot analysis.\n",
    "\n",
    "The classification task presented several inherent challenges that significantly impacted model performance. First, the synthetic nature of the dataset, while providing balanced class distribution and complete coverage of feature ranges, may lack the complex, non-linear relationships and interactions that characterize real-world agricultural systems where multiple factors interact in ways that cannot be captured by simple uniform distributions. Second, the choice to convert continuous yield values into three discrete categories necessarily loses information, as the boundaries between Low, Medium, and High yields are somewhat arbitrary despite being based on percentiles. Third, the Medium yield category proved particularly difficult to classify across all three algorithms, as instances near the 33rd and 67th percentile boundaries share characteristics with both adjacent categories, creating inherent ambiguity. Finally, our use of only eight numeric features, while computationally manageable, may have excluded important predictors of crop yield such as crop-specific characteristics, detailed soil chemistry, pest pressure, or historical yield trends that would be present in real agricultural datasets.\n",
    "\n",
    "We developed and implemented three classification algorithms from scratch following CPSC 322 requirements, using only Python's csv module and matplotlib for data handling and visualization. Our Random Forest classifier employed an ensemble of 10 decision trees with bootstrap sampling and random feature selection (square root of total features) at each split, limited to a maximum depth of 5 levels. The k-Nearest Neighbors algorithm used k=5 neighbors with Euclidean distance, selected specifically to evaluate how instance-based learning would scale to our 10,000-instance dataset. Naive Bayes required discretizing continuous features into Low, Medium, and High bins, which we accomplished using equal-width binning based on the range of each feature, and we hypothesized this approach would perform best due to its efficient probabilistic framework for handling multiple features. All classifiers were evaluated using a 67-33 train-test split with accuracy as the primary metric and confusion matrices to assess per-class performance.\n",
    "\n",
    "The performance results confirmed our hypothesis that Naive Bayes would be the strongest performer, achieving 47.27% accuracy and significantly outperforming k-NN (37.48%) and Random Forest (32.82%). However, all three classifiers achieved accuracy rates below 50%, which is only moderately better than random guessing for a three-class problem (33.3% baseline). Naive Bayes demonstrated exceptional performance on extreme categories (Low: 51.6%, High: 87.1%) but struggled dramatically with the Medium category (3.0%), suggesting that after discretization, the middle category shares probabilistic features with both extremes. The k-NN classifier showed more balanced per-class performance but was computationally expensive due to distance calculations across the entire training set. Random Forest's poor performance was unexpected for an ensemble method and may indicate that our maximum depth restriction was too conservative or that the synthetic data lacks the complex feature interactions that typically benefit ensemble approaches.\n",
    "\n",
    "Several strategies could potentially improve classification performance in future work. First, implementing feature engineering to create interaction terms (such as temperature √ó precipitation or soil health √ó fertilizer use) might capture the synergistic effects that characterize real agricultural systems. Second, using stratified sampling rather than random sampling for the train-test split would ensure that each class is proportionally represented in both sets, which could improve model training, particularly for the problematic Medium category. Third, conducting feature selection or dimensionality reduction could identify the most predictive variables and reduce noise from less informative features. Fourth, for Naive Bayes specifically, experimenting with different discretization strategies such as quantile-based binning (using percentiles rather than equal-width bins) might create more meaningful categorical distinctions that better separate the classes. Fifth, significantly increasing the number of trees in Random Forest (from 10 to 50 or 100) and relaxing the maximum depth constraint could allow the ensemble to capture more complex patterns, though this would increase computational cost. Sixth, implementing cross-validation rather than a single train-test split would provide more robust estimates of model performance and reduce the impact of random variation in data splitting. Finally, if this were a real-world application, collecting additional features such as crop-specific growth characteristics, detailed soil composition data, historical weather patterns, or economic factors like market prices and input costs could provide the additional information necessary to achieve the higher accuracy rates required for practical agricultural decision-making. Despite the modest accuracy achieved, this project successfully demonstrated the implementation and comparison of three distinct machine learning paradigms‚Äîensemble learning, instance-based learning, and probabilistic learning‚Äîand provided valuable insights into the challenges of multi-class classification on balanced synthetic datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1d0a8831b7354a",
   "metadata": {},
   "source": [
    "# Acknowledgements\n",
    "\n",
    "Claude AI was used for assistance in this project for helping understanding and developing Random Forest and its unit tests, correcting bugs in our EDA code, and assisting with our report notebook. Our Naive Bayes and kNN classifier code was taken from previous PAs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
