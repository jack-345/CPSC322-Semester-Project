{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b9abaefc4eec3c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T06:23:37.870212Z",
     "start_time": "2025-12-10T06:23:37.865821Z"
    }
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import mysklearn\n",
    "importlib.reload(mysklearn)\n",
    "\n",
    "import mysklearn.mypytable\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable\n",
    "\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyNaiveBayesClassifier\n",
    "from mysklearn.myclassifiers import MyDecisionTreeClassifier\n",
    "from mysklearn.myclassifiers import MyRandomForestClassifier\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198bedb419537b78",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "For this project, we used a fully synthetic dataset from Kaggle. It contains mostly continuous data. It has 15 total attributes and 10,000 instances. We tried to classify if a crop yield was Low, Medium, or High as labels because there was no existing attribute appropriate for prediction.\n",
    "\n",
    "(findings here)\n",
    "(best performing classifier)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875f34c98f561136",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "Our dataset is mostly continuous with 10,000 instances and 15 attributes. The \"Year\" attribute is an integer representing the year of recorded instance values. The attributes Country,Region,Crop_Type, and Adaptation_Strategy are all categorical strings. The attributes Average_Temperature_C, Total_Precipitation_mm, CO2_Emissions_MT, Crop_Yield_MT_per_HA, Extreme_Weather_Events, Irrigation_Access_%, Pesticide_Use_KG_per_HA, Fertilizer_Use_KG_per_HA, Soil_Health_Index, and Economic_Impact_Million_USD are all float values.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89036074",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/msys64/ucrt64/bin/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#Summary Statistics \n",
    "\n",
    "import csv\n",
    "\n",
    "def load_data(filename):\n",
    "    table = []\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)\n",
    "        for row in reader:\n",
    "            table.append(row)\n",
    "    return header, table\n",
    "\n",
    "def compute_summary_stats(values):\n",
    "    \"\"\"Calculate summary statistics for a list of numeric values.\"\"\"\n",
    "    sorted_vals = sorted(values)\n",
    "    n = len(sorted_vals)\n",
    "    \n",
    "    \n",
    "    min_val = sorted_vals[0]\n",
    "    max_val = sorted_vals[-1]\n",
    "    range_val = max_val - min_val\n",
    "    mean = sum(sorted_vals) / n\n",
    "    \n",
    "    \n",
    "    if n % 2 == 0:\n",
    "        median = (sorted_vals[n//2 - 1] + sorted_vals[n//2]) / 2\n",
    "    else:\n",
    "        median = sorted_vals[n//2]\n",
    "    \n",
    "    \n",
    "    q1_idx = n // 4\n",
    "    q3_idx = (3 * n) // 4\n",
    "    q1 = sorted_vals[q1_idx]\n",
    "    q3 = sorted_vals[q3_idx]\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    \n",
    "    variance = sum((x - mean) ** 2 for x in sorted_vals) / n\n",
    "    std_dev = variance ** 0.5\n",
    "    \n",
    "    return {\n",
    "        'count': n,\n",
    "        'min': min_val,\n",
    "        'max': max_val,\n",
    "        'range': range_val,\n",
    "        'mean': mean,\n",
    "        'median': median,\n",
    "        'std': std_dev,\n",
    "        'Q1': q1,\n",
    "        'Q2': median,\n",
    "        'Q3': q3,\n",
    "        'IQR': iqr\n",
    "    }\n",
    "\n",
    "\n",
    "filename = 'climate_change_impact_on_agriculture_2024.csv'\n",
    "header, table = load_data(filename)\n",
    "\n",
    "\n",
    "numeric_features = [\n",
    "    'Average_Temperature_C', 'Total_Precipitation_mm', \n",
    "    'CO2_Emissions_MT', 'Crop_Yield_MT_per_HA',\n",
    "    'Extreme_Weather_Events', 'Irrigation_Access_%', \n",
    "    'Pesticide_Use_KG_per_HA', 'Fertilizer_Use_KG_per_HA', \n",
    "    'Soil_Health_Index'\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY STATISTICS FOR NUMERIC FEATURES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for feat_name in numeric_features:\n",
    "    feat_idx = header.index(feat_name)\n",
    "    values = []\n",
    "    for row in table:\n",
    "        try:\n",
    "            values.append(float(row[feat_idx]))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    stats = compute_summary_stats(values)\n",
    "    \n",
    "    print(f\"\\n{feat_name}:\")\n",
    "    print(f\"  Count:  {stats['count']}\")\n",
    "    print(f\"  Mean:   {stats['mean']:.2f}\")\n",
    "    print(f\"  Median: {stats['median']:.2f}\")\n",
    "    print(f\"  Std:    {stats['std']:.2f}\")\n",
    "    print(f\"  Min:    {stats['min']:.2f}\")\n",
    "    print(f\"  Q1:     {stats['Q1']:.2f}\")\n",
    "    print(f\"  Q2:     {stats['Q2']:.2f}\")\n",
    "    print(f\"  Q3:     {stats['Q3']:.2f}\")\n",
    "    print(f\"  Max:    {stats['max']:.2f}\")\n",
    "    print(f\"  Range:  {stats['range']:.2f}\")\n",
    "    print(f\"  IQR:    {stats['IQR']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bdba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "print(\"Figure 1: Target Variable Distribution\")\n",
    "display(Image('figures/figure1_target.png'))\n",
    "\n",
    "print(\"\\nFigure 2: Climate Variables Distribution\")\n",
    "display(Image('figures/figure2_climate.png'))\n",
    "\n",
    "print(\"\\nFigure 3: Agricultural Variables Distribution\")\n",
    "display(Image('figures/figure3_agriculture.png'))\n",
    "\n",
    "print(\"\\nFigure 4: Categorical Variables Distribution\")\n",
    "display(Image('figures/figure4_categorical.png'))\n",
    "\n",
    "print(\"\\nFigure 5: Box and Whisker Plots\")\n",
    "display(Image('figures/figure5_boxplots.png'))\n",
    "\n",
    "print(\"\\nFigure 6: Scatter Plots - Variable Relationships\")\n",
    "display(Image('figures/figure6_scatter.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e63871a",
   "metadata": {},
   "source": [
    "Figure 1: Target Variable Distributions\n",
    "Figure 1 shows the distribution of our target variable, Crop Yield. The left histogram displays the continuous yield values ranging from approximately 0.86 to 5 MT/HA, with vertical lines marking the 33rd percentile (1.68 MT/HA) and 67th percentile (2.66 MT/HA) that we used as category boundaries. The right bar chart confirms our categorization resulted in a balanced three-class problem with Low (33.1%), Medium (33.9%), and High (33.1%) yields approximately equally represented. This balanced distribution is crucial for training unbiased classification models, as it ensures that no single class dominates the dataset and that our classifiers will have equal opportunity to learn patterns for all three yield categories.\n",
    "\n",
    "Figure 2:\n",
    "Figure 2 presents the distributions of four key climate variables in our dataset. The temperature distribution shows values ranging from approximately -5¬∞C to 35¬∞C with a relatively uniform spread, though there is a noticeable gap in data around 10-20¬∞C which may indicate either missing data or regional climate patterns where certain temperature ranges are less common in agricultural areas. Precipitation displays an even distribution from 0 to 3000mm annually, suggesting the dataset captures diverse climatic conditions from arid to high-rainfall regions. CO2 emissions show a uniform distribution between 0 and 30 MT, indicating consistent representation across different emission levels. Extreme weather events are also uniformly distributed from 0 to 6 events, with all frequency levels equally represented. The uniformity of these distributions suggests the dataset was carefully balanced or synthetically generated to ensure equal representation across all climate conditions, which is beneficial for training classifiers that can generalize across diverse environmental scenarios.\n",
    "\n",
    "Figure 3:\n",
    "Figure 3 displays the distributions of four agricultural practice variables: irrigation access, pesticide use, fertilizer use, and soil health index. Irrigation access shows a uniform distribution across the full range from 0% to 100%, indicating the dataset includes farms with no irrigation infrastructure as well as those with complete irrigation coverage. Pesticide use is evenly distributed from 0 to 50 KG/HA, representing diverse farming approaches from organic or low-input systems to conventional high-input agriculture. Fertilizer use similarly shows uniform distribution from 0 to 100 KG/HA, capturing the full spectrum of nutrient management practices. The soil health index ranges from approximately 30 to 100 with consistent frequency across all values, representing soil conditions from poor to excellent. The remarkably uniform distributions across all four variables suggest this dataset was synthetically generated or carefully balanced to ensure equal representation of different agricultural practices. This uniformity is advantageous for machine learning as it provides our classifiers with sufficient examples across the entire range of each variable, enabling them to learn patterns without bias toward any particular farming practice level.\n",
    "\n",
    "Figure 4: \n",
    "Figure 4 examines the distribution of four categorical variables in the dataset: country, region, crop type, and adaptation strategies. The country distribution shows that the dataset includes ten major agricultural nations, with USA, Australia, and China being the most represented (approximately 1000 instances each), followed by Nigeria, India, Canada, Argentina, France, Russia, and Brazil with roughly equal representation. This geographic diversity ensures the dataset captures agricultural practices across different continents and climate zones. The regional distribution reveals that South and Northeast regions dominate with approximately 750 instances each, followed by North, Central, Punjab, Victoria, New South Wales, East, South West, and Ontario regions. The crop type distribution demonstrates remarkable diversity, with ten major crops represented almost equally: wheat, cotton, vegetables, corn, rice, sugarcane, fruits, soybeans, barley, and coffee, each appearing approximately 1000 times. This balanced representation across diverse crops from grains to cash crops to vegetables ensures our classifiers can learn patterns applicable to multiple agricultural contexts. Finally, the adaptation strategies variable shows that water management is by far the most common strategy with approximately 2000 instances, followed by no adaptation, drought-resistant crops, organic farming, and crop rotation with roughly equal representation around 1500 instances each. The prevalence of water management strategies reflects the critical\n",
    "\n",
    "Figure 6:\n",
    "Figure 6 presents scatter plots showing the relationships between four key variables and crop yield, with points color-coded by yield category (red for Low, yellow for Medium, green for High). The temperature versus yield plot reveals a notable pattern with a conspicuous gap in the 10-20¬∞C range where no data points appear, suggesting either missing data or that crops in this dataset are not grown in that temperature range. Higher temperatures (20-35¬∞C) are associated with higher yields, as evidenced by the concentration of green points in this region. The precipitation versus yield plot shows a more uniform distribution across all precipitation levels (0-3000mm), with all three yield categories well-mixed throughout the range, suggesting that precipitation alone is not a strong predictor of yield category. The soil health versus yield plot demonstrates the clearest separation among all four plots, with low yields (red) predominantly appearing in the 30-60 soil health range, medium yields (yellow) in the 50-80 range, and high yields (green) concentrated in the 60-100 range. This strong visual separation indicates that soil health is likely to be one of the most important features for our classification models. The irrigation access versus yield plot shows moderate separation, with low yields more common at lower irrigation levels and high yields appearing across all irrigation levels but with slightly higher concentration at higher access percentages. These scatter plots provide crucial insights into which features will be most predictive for classifying crop yields, with soil health emerging as the strongest individual predictor, followed by temperature and irrigation access, while precipitation shows weaker discriminative power.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70ba0ccb8082e95",
   "metadata": {},
   "source": [
    "# Classification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "781e7e40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T06:23:39.926030Z",
     "start_time": "2025-12-10T06:23:39.923829Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from mysklearn.myclassifiers import MyRandomForestClassifier, MyKNeighborsClassifier\n",
    "from mysklearn.myevaluation import train_test_split, confusion_matrix, accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48922679",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T06:23:41.810029Z",
     "start_time": "2025-12-10T06:23:41.805166Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    table = []\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)\n",
    "        for row in reader:\n",
    "            table.append(row)\n",
    "    return header, table\n",
    "\n",
    "def prepare_data(header, table):\n",
    "    numeric_features = ['Average_Temperature_C', 'Total_Precipitation_mm', \n",
    "                       'CO2_Emissions_MT', 'Extreme_Weather_Events',\n",
    "                       'Irrigation_Access_%', 'Pesticide_Use_KG_per_HA', \n",
    "                       'Fertilizer_Use_KG_per_HA', 'Soil_Health_Index']\n",
    "    \n",
    "    feature_indices = [header.index(feat) for feat in numeric_features]\n",
    "    yield_index = header.index('Crop_Yield_MT_per_HA')\n",
    "    \n",
    "    X = []\n",
    "    y_continuous = []\n",
    "    for row in table:\n",
    "        try:\n",
    "            features = [float(row[i]) for i in feature_indices]\n",
    "            yield_val = float(row[yield_index])\n",
    "            X.append(features)\n",
    "            y_continuous.append(yield_val)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    sorted_yields = sorted(y_continuous)\n",
    "    p33_index = int(len(sorted_yields) * 0.33)\n",
    "    p67_index = int(len(sorted_yields) * 0.67)\n",
    "    p33 = sorted_yields[p33_index]\n",
    "    p67 = sorted_yields[p67_index]\n",
    "    \n",
    "    y = []\n",
    "    for yield_val in y_continuous:\n",
    "        if yield_val < p33:\n",
    "            y.append('Low')\n",
    "        elif yield_val < p67:\n",
    "            y.append('Medium')\n",
    "        else:\n",
    "            y.append('High')\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def print_confusion_matrix(matrix, labels):\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"{'':12}\", end=\"\")\n",
    "    for label in labels:\n",
    "        print(f\"{label:>10}\", end=\"\")\n",
    "    print()\n",
    "    print(\"-\" * 50)\n",
    "    for i, label in enumerate(labels):\n",
    "        print(f\"{label:12}\", end=\"\")\n",
    "        for j in range(len(labels)):\n",
    "            print(f\"{matrix[i][j]:>10}\", end=\"\")\n",
    "        print()\n",
    "\n",
    "def discretize_features(X):\n",
    "    \"\"\"Convert continuous features to categorical bins for Naive Bayes.\"\"\"\n",
    "    X_discretized = []\n",
    "    \n",
    "    # First, find min/max for each feature to create bins\n",
    "    n_features = len(X[0])\n",
    "    feature_mins = [min(instance[i] for instance in X) for i in range(n_features)]\n",
    "    feature_maxs = [max(instance[i] for instance in X) for i in range(n_features)]\n",
    "    \n",
    "    for instance in X:\n",
    "        discretized_instance = []\n",
    "        for i, value in enumerate(instance):\n",
    "            # Create 3 equal-width bins: Low, Medium, High\n",
    "            range_size = (feature_maxs[i] - feature_mins[i]) / 3\n",
    "            if value < feature_mins[i] + range_size:\n",
    "                discretized_instance.append('Low')\n",
    "            elif value < feature_mins[i] + 2 * range_size:\n",
    "                discretized_instance.append('Medium')\n",
    "            else:\n",
    "                discretized_instance.append('High')\n",
    "        X_discretized.append(discretized_instance)\n",
    "    \n",
    "    return X_discretized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fac577a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T06:23:45.000012Z",
     "start_time": "2025-12-10T06:23:44.943621Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 10000 instances, 8 features\n",
      "Classes: Low, Medium, High\n",
      "Training: 6700 instances\n",
      "Test: 3300 instances\n"
     ]
    }
   ],
   "source": [
    "filename = 'climate_change_impact_on_agriculture_2024.csv'\n",
    "header, table = load_data(filename)\n",
    "X, y = prepare_data(header, table)\n",
    "\n",
    "print(f\"Dataset: {len(X)} instances, {len(X[0])} features\")\n",
    "print(f\"Classes: Low, Medium, High\")\n",
    "\n",
    "# Split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "print(f\"Training: {len(X_train)} instances\")\n",
    "print(f\"Test: {len(X_test)} instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d0fb950",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T06:23:55.844564Z",
     "start_time": "2025-12-10T06:23:46.910395Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RANDOM FOREST CLASSIFIER\n",
      "============================================================\n",
      "Accuracy: 0.3282 (32.82%)\n",
      "\n",
      "Confusion Matrix:\n",
      "==================================================\n",
      "                   Low    Medium      High\n",
      "--------------------------------------------------\n",
      "Low                662       207       196\n",
      "Medium             698       235       180\n",
      "High               709       227       186\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"RANDOM FOREST CLASSIFIER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "rf = MyRandomForestClassifier(n_trees=10, max_depth=5)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "rf_acc = accuracy_score(y_test, rf_pred)\n",
    "\n",
    "print(f\"Accuracy: {rf_acc:.4f} ({rf_acc*100:.2f}%)\")\n",
    "\n",
    "labels = ['Low', 'Medium', 'High']\n",
    "rf_matrix = confusion_matrix(y_test, rf_pred, labels)\n",
    "print_confusion_matrix(rf_matrix, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc5c9aa7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T06:24:19.444655Z",
     "start_time": "2025-12-10T06:23:59.553950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "K-NEAREST NEIGHBORS\n",
      "============================================================\n",
      "Accuracy: 0.3748 (37.48%)\n",
      "\n",
      "Confusion Matrix:\n",
      "==================================================\n",
      "                   Low    Medium      High\n",
      "--------------------------------------------------\n",
      "Low                349       377       339\n",
      "Medium             331       411       371\n",
      "High               277       368       477\n",
      "\n",
      "============================================================\n",
      "NAIVE BAYES CLASSIFIER\n",
      "============================================================\n",
      "Discretizing continuous features into Low/Medium/High bins...\n",
      "\n",
      "‚úì Naive Bayes Accuracy: 0.4727 (47.27%)\n",
      "\n",
      "Confusion Matrix:\n",
      "==================================================\n",
      "                   Low    Medium      High\n",
      "--------------------------------------------------\n",
      "Low                550        37       478\n",
      "Medium             442        33       638\n",
      "High               113        32       977\n",
      "\n",
      "Per-Class Recognition Rates:\n",
      "  Low: 550/1065 = 51.6%\n",
      "  Medium: 33/1113 = 3.0%\n",
      "  High: 977/1122 = 87.1%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"K-NEAREST NEIGHBORS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "knn = MyKNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "knn_pred = knn.predict(X_test)\n",
    "knn_acc = accuracy_score(y_test, knn_pred)\n",
    "\n",
    "print(f\"Accuracy: {knn_acc:.4f} ({knn_acc*100:.2f}%)\")\n",
    "\n",
    "knn_matrix = confusion_matrix(y_test, knn_pred, labels)\n",
    "print_confusion_matrix(knn_matrix, labels)\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NAIVE BAYES CLASSIFIER\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Discretizing continuous features into Low/Medium/High bins...\")\n",
    "\n",
    "# Discretize features for Naive Bayes\n",
    "X_train_disc = discretize_features(X_train)\n",
    "X_test_disc = discretize_features(X_test)\n",
    "\n",
    "# Train Naive Bayes\n",
    "nb = MyNaiveBayesClassifier()\n",
    "nb.fit(X_train_disc, y_train)\n",
    "nb_pred = nb.predict(X_test_disc)\n",
    "nb_acc = accuracy_score(y_test, nb_pred)\n",
    "\n",
    "print(f\"\\n‚úì Naive Bayes Accuracy: {nb_acc:.4f} ({nb_acc*100:.2f}%)\")\n",
    "\n",
    "nb_matrix = confusion_matrix(y_test, nb_pred, labels)\n",
    "print_confusion_matrix(nb_matrix, labels)\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "print(\"\\nPer-Class Recognition Rates:\")\n",
    "for i, label in enumerate(labels):\n",
    "    total = sum(nb_matrix[i])\n",
    "    correct = nb_matrix[i][i]\n",
    "    rate = (correct / total * 100) if total > 0 else 0\n",
    "    print(f\"  {label}: {correct}/{total} = {rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0403b688",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T06:24:28.608728Z",
     "start_time": "2025-12-10T06:24:28.603794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FINAL COMPARISON\n",
      "======================================================================\n",
      "\n",
      "Overall Accuracy:\n",
      "  Random Forest:  0.3282 (32.82%)\n",
      "  k-NN (k=5):     0.3748 (37.48%)\n",
      "  Naive Bayes:    0.4727 (47.27%)\n",
      "\n",
      "üèÜ Rankings:\n",
      "  1. Naive Bayes: 0.4727 (47.27%)\n",
      "  2. k-NN: 0.3748 (37.48%)\n",
      "  3. Random Forest: 0.3282 (32.82%)\n",
      "\n",
      "üèÜ Winner: Naive Bayes with 47.27% accuracy!\n",
      "\n",
      "Performance Gaps:\n",
      "  1st vs 2nd: 9.79 percentage points\n",
      "  1st vs 3rd: 14.45 percentage points\n",
      "\n",
      "======================================================================\n",
      "ANALYSIS COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nOverall Accuracy:\")\n",
    "print(f\"  Random Forest:  {rf_acc:.4f} ({rf_acc*100:.2f}%)\")\n",
    "print(f\"  k-NN (k=5):     {knn_acc:.4f} ({knn_acc*100:.2f}%)\")\n",
    "print(f\"  Naive Bayes:    {nb_acc:.4f} ({nb_acc*100:.2f}%)\")\n",
    "\n",
    "# Find winner\n",
    "accuracies = [('Random Forest', rf_acc), ('k-NN', knn_acc), ('Naive Bayes', nb_acc)]\n",
    "accuracies_sorted = sorted(accuracies, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\nüèÜ Rankings:\")\n",
    "for i, (name, acc) in enumerate(accuracies_sorted, 1):\n",
    "    print(f\"  {i}. {name}: {acc:.4f} ({acc*100:.2f}%)\")\n",
    "\n",
    "winner = accuracies_sorted[0]\n",
    "print(f\"\\nüèÜ Winner: {winner[0]} with {winner[1]*100:.2f}% accuracy!\")\n",
    "\n",
    "# Show differences\n",
    "print(f\"\\nPerformance Gaps:\")\n",
    "print(f\"  1st vs 2nd: {(accuracies_sorted[0][1] - accuracies_sorted[1][1])*100:.2f} percentage points\")\n",
    "print(f\"  1st vs 3rd: {(accuracies_sorted[0][1] - accuracies_sorted[2][1])*100:.2f} percentage points\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95091b7bf40d8e36",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1d0a8831b7354a",
   "metadata": {},
   "source": [
    "# Acknowledgements\n",
    "\n",
    "Claude AI was used for assistance in this project for helping understanding and developing Random Forest and its unit tests, correcting bugs in our EDA code, and assisting with our report notebook. Our Naive Bayes and kNN classifier code was taken from previous PAs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
